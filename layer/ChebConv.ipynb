{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-17T09:14:18.467992Z",
     "iopub.status.busy": "2024-03-17T09:14:18.467625Z",
     "iopub.status.idle": "2024-03-17T09:14:22.777060Z",
     "shell.execute_reply": "2024-03-17T09:14:22.776210Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda True\n",
      "NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorboard\n",
    "from rdkit import Chem\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, Timer\n",
    "\n",
    "from lion_pytorch import Lion\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"cuda\", torch.cuda.is_available())\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pytorch_lightning.trainer.connectors.data_connector\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"lightning_fabric.plugins.environments.slurm\")\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from utils.train import MoleculeModel, MoleculeDataModule, evaluate_model\n",
    "from utils.prepare import MoleculeData, MoleculeDataset, FeaturizationParameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GATv2Conv, TransformerConv, ChebConv\n",
    "from torch_scatter import scatter_mean\n",
    "from torch.utils.data import Subset\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_module):\n",
    "    test_dl = data_module.test_dataloader()\n",
    "    model.eval()  \n",
    "    all_pred, all_true = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dl:\n",
    "            y_hat = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "            all_pred.extend(y_hat.cpu().numpy())\n",
    "            all_true.extend(batch.y.cpu().numpy())\n",
    "\n",
    "    all_pred, all_true = np.array(all_pred), np.array(all_true)\n",
    "    rmse = np.sqrt(mean_squared_error(all_true, all_pred))\n",
    "    r2 = r2_score(all_true, all_pred)\n",
    "\n",
    "    print(f'Test RMSE: {rmse:.4f}')\n",
    "    print(f'Test RÂ²: {r2:.4f}')\n",
    "\n",
    "def create_hyperopt_dir(base_dir='hyperopt_'):\n",
    "    idx = 1\n",
    "    while True:\n",
    "        dir_name = f\"{base_dir}{idx}\"\n",
    "        if not os.path.exists(dir_name):\n",
    "            os.makedirs(dir_name)\n",
    "            return dir_name\n",
    "        idx += 1\n",
    "\n",
    "\n",
    "def save_trial_to_csv(trial, hyperopt_dir, trial_value):\n",
    "    csv_path = os.path.join(hyperopt_dir, 'optuna_results.csv')\n",
    "    with open(csv_path, 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        if os.path.getsize(csv_path) == 0:  \n",
    "            headers = ['Trial'] + ['Value'] + [key for key in trial.params.keys()]\n",
    "            writer.writerow(headers)\n",
    "        row = [trial.number] + [trial_value] + list(trial.params.values())\n",
    "        writer.writerow(row)\n",
    "\n",
    "\n",
    "\n",
    "class MoleculeDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, dataset, batch_size=128, val_split=0.1, test_split=0.2, num_workers=1):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.val_split = val_split\n",
    "        self.test_split = test_split\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        indices = list(range(len(self.dataset)))\n",
    "        train_val_indices, test_indices = train_test_split(indices, test_size=self.test_split, random_state=42)\n",
    "        train_indices, val_indices = train_test_split(train_val_indices, test_size=self.val_split / (1 - self.test_split), random_state=42)\n",
    "        \n",
    "        self.train_dataset = Subset(self.dataset, train_indices)\n",
    "        self.val_dataset = Subset(self.dataset, val_indices)\n",
    "        self.test_dataset = Subset(self.dataset, test_indices)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "\n",
    "\n",
    "\n",
    "class MoleculeModel(pl.LightningModule):\n",
    "    def __init__(self, atom_in_features, preprocess_hidden_features, cheb_hidden_features, K, cheb_normalizations, dropout_rates, activation_fns, use_batch_norm, postprocess_hidden_features, out_features, optimizer_class, learning_rate, weight_decay, step_size, gamma, batch_size, metric='rmse'):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.batch_size = batch_size\n",
    "        self.metric = self.get_metric(metric)\n",
    "        \n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "        # Preprocessing layers for atom features\n",
    "        self.atom_preprocess = nn.ModuleList()\n",
    "        for i in range(len(preprocess_hidden_features)):\n",
    "            preprocess_layer = nn.Sequential()\n",
    "            in_features = atom_in_features if i == 0 else preprocess_hidden_features[i-1]\n",
    "            preprocess_layer.add_module(f'atom_linear_{i}', nn.Linear(in_features, preprocess_hidden_features[i]))\n",
    "            if use_batch_norm[i]:\n",
    "                preprocess_layer.add_module(f'atom_bn_{i}', nn.BatchNorm1d(preprocess_hidden_features[i]))\n",
    "            preprocess_layer.add_module(f'atom_activation_{i}', activation_fns[i]())\n",
    "            preprocess_layer.add_module(f'atom_dropout_{i}', nn.Dropout(dropout_rates[i]))\n",
    "            self.atom_preprocess.append(preprocess_layer)\n",
    "\n",
    "        # Chebyshev convolution layers\n",
    "        self.cheb_convolutions = nn.ModuleList()\n",
    "        in_channels = preprocess_hidden_features[-1]\n",
    "        for i in range(len(cheb_hidden_features)):\n",
    "            out_channels = cheb_hidden_features[i]\n",
    "            normalization = cheb_normalizations[i] if i < len(cheb_normalizations) else 'none'  # Default to 'none' if not specified\n",
    "            self.cheb_convolutions.append(ChebConv(in_channels=in_channels, out_channels=out_channels, K=K[i], normalization=normalization))\n",
    "            in_channels = out_channels  # Update in_channels for the next layer\n",
    "\n",
    "\n",
    "        # Postprocessing layers\n",
    "        self.postprocess = nn.ModuleList()\n",
    "        in_features = cheb_hidden_features[-1]  # Start from the output of the last ChebConv layer\n",
    "        for i in range(len(postprocess_hidden_features)):\n",
    "            post_layer = nn.Sequential()\n",
    "            post_layer.add_module(f'post_linear_{i}', nn.Linear(in_features, postprocess_hidden_features[i]))\n",
    "            if use_batch_norm[len(preprocess_hidden_features) + i]:\n",
    "                post_layer.add_module(f'post_bn_{i}', nn.BatchNorm1d(postprocess_hidden_features[i]))\n",
    "            post_layer.add_module(f'post_activation_{i}', activation_fns[len(preprocess_hidden_features) + i]())\n",
    "            post_layer.add_module(f'post_dropout_{i}', nn.Dropout(dropout_rates[len(preprocess_hidden_features) + i]))\n",
    "            self.postprocess.append(post_layer)\n",
    "            in_features = postprocess_hidden_features[i]  # Update in_features for the next layer\n",
    "\n",
    "        self.output_layer = nn.Linear(postprocess_hidden_features[-1], out_features)\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for layer in self.atom_preprocess:\n",
    "            x = layer(x)\n",
    "\n",
    "        for conv in self.cheb_convolutions:\n",
    "            x = F.relu(conv(x, edge_index))\n",
    "\n",
    "        for layer in self.postprocess:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.output_layer(x).squeeze(-1)\n",
    "        return x\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = self.hparams.optimizer_class(self.parameters(), lr=self.hparams.learning_rate, weight_decay=self.hparams.weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=self.hparams.step_size, gamma=self.hparams.gamma)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def on_train_start(self) -> None:\n",
    "        for name, module in self.named_modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                module.register_forward_hook(self.log_activations_hook(name))\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        y_hat = self(batch.x, batch.edge_index)\n",
    "        loss = self.metric(batch.y, y_hat)\n",
    "        self.log('train_loss', loss, batch_size=self.batch_size, on_step=True, on_epoch=True, prog_bar=True, logger=True, enable_graph=True)\n",
    "        self.train_losses.append(loss.item())\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        y_hat = self(batch.x, batch.edge_index)\n",
    "        val_loss = self.metric(batch.y, y_hat)\n",
    "        self.log('val_loss', val_loss, batch_size=self.batch_size, on_step=True, on_epoch=True, prog_bar=True, logger=True, enable_graph=True)\n",
    "        self.val_losses.append(val_loss.item())\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        y_hat = self(batch.x, batch.edge_index)\n",
    "        preds_np = y_hat.detach().cpu().numpy()\n",
    "        true_values_np = batch.y.detach().cpu().numpy()\n",
    "\n",
    "        data = []\n",
    "        start_idx = 0\n",
    "        for i, num_atoms in enumerate(batch.ptr[:-1]): \n",
    "            end_idx = batch.ptr[i+1].item()\n",
    "            molecule_preds = preds_np[start_idx:end_idx]\n",
    "            molecule_true_values = true_values_np[start_idx:end_idx]\n",
    "\n",
    "            data.append({\n",
    "                'smiles': batch.smiles[i],\n",
    "                'predictions': molecule_preds,\n",
    "                'true_values': molecule_true_values\n",
    "            })\n",
    "\n",
    "            start_idx = end_idx\n",
    "        return data\n",
    "\n",
    "    def on_test_epoch_end(self, outputs):\n",
    "\n",
    "        all_data = [item for batch_data in outputs for item in batch_data]\n",
    "        self.df_results = pd.DataFrame(all_data)\n",
    "\n",
    "        all_predictions = np.concatenate(self.df_results['predictions'].values)\n",
    "        all_true_values = np.concatenate(self.df_results['true_values'].values)\n",
    "\n",
    "        rmse = np.sqrt(mean_squared_error(all_true_values, all_predictions))\n",
    "        mse = mean_squared_error(all_true_values, all_predictions)\n",
    "        r2 = r2_score(all_true_values, all_predictions)\n",
    "        mae = mean_absolute_error(all_true_values, all_predictions)\n",
    "\n",
    "        self.log('test_rmse', rmse)\n",
    "        self.log('test_mse', mse)\n",
    "        self.log('test_r2', r2)\n",
    "        self.log('test_mae', mae)\n",
    "\n",
    "        print(f'Test RMSE: {rmse:.4f}')\n",
    "        print(f'Test MSE: {mse:.4f}')\n",
    "        print(f'Test RÂ²: {r2:.4f}')\n",
    "        print(f'Test MAE: {mae:.4f}')\n",
    "\n",
    "        return self.df_results\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            self.logger.experiment.add_histogram(name, param, self.current_epoch)\n",
    "            \n",
    "    def log_activations_hook(self, layer_name):\n",
    "        def hook(module, input, output):\n",
    "            self.logger.experiment.add_histogram(f\"{layer_name}_activations\", output, self.current_epoch)\n",
    "        return hook\n",
    "\n",
    "    def get_metric(self, metric_name):\n",
    "        if metric_name == 'mse':\n",
    "            def mse(y_true, y_pred):\n",
    "                return F.mse_loss(y_pred, y_true)\n",
    "            return mse\n",
    "\n",
    "        elif metric_name == 'rmse':\n",
    "            def rmse(y_true, y_pred):\n",
    "                return torch.sqrt(F.mse_loss(y_pred, y_true))\n",
    "            return rmse\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"ÐÐµÐ¸Ð·Ð²ÐµÑÑÐ½Ð¾Ðµ Ð¸Ð¼Ñ Ð¼ÐµÑÑÐ¸ÐºÐ¸: {metric_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-17T09:14:22.781643Z",
     "iopub.status.busy": "2024-03-17T09:14:22.781416Z",
     "iopub.status.idle": "2024-03-17T09:15:53.120597Z",
     "shell.execute_reply": "2024-03-17T09:15:53.111858Z"
    }
   },
   "outputs": [],
   "source": [
    "molecule_dataset = torch.load(\"../data/QM_137k.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-17T09:15:53.133986Z",
     "iopub.status.busy": "2024-03-17T09:15:53.133560Z",
     "iopub.status.idle": "2024-03-17T09:15:53.142927Z",
     "shell.execute_reply": "2024-03-17T09:15:53.142248Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[31, 333], edge_index=[2, 64], edge_attr=[64, 414], y=[31], smiles='CNC(=S)N/N=C/c1c(O)ccc2ccccc12')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "molecule_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-17T09:15:53.150721Z",
     "iopub.status.busy": "2024-03-17T09:15:53.150370Z",
     "iopub.status.idle": "2024-03-17T09:15:53.154759Z",
     "shell.execute_reply": "2024-03-17T09:15:53.154105Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 128   \n",
    "num_workers = 8  \n",
    "\n",
    "data_module = MoleculeDataModule(molecule_dataset, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-17T09:15:53.200256Z",
     "iopub.status.busy": "2024-03-17T09:15:53.199970Z",
     "iopub.status.idle": "2024-03-17T09:15:53.363017Z",
     "shell.execute_reply": "2024-03-17T09:15:53.362347Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\n",
      " MoleculeModel(\n",
      "  (atom_preprocess): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (atom_linear_0): Linear(in_features=333, out_features=128, bias=True)\n",
      "      (atom_bn_0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (atom_activation_0): PReLU(num_parameters=1)\n",
      "      (atom_dropout_0): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (atom_linear_1): Linear(in_features=128, out_features=256, bias=True)\n",
      "      (atom_bn_1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (atom_activation_1): PReLU(num_parameters=1)\n",
      "      (atom_dropout_1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (atom_linear_2): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (atom_bn_2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (atom_activation_2): PReLU(num_parameters=1)\n",
      "      (atom_dropout_2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (cheb_convolutions): ModuleList(\n",
      "    (0-1): 2 x ChebConv(128, 128, K=10, normalization=sym)\n",
      "  )\n",
      "  (postprocess): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (post_linear_0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (post_bn_0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (post_activation_0): PReLU(num_parameters=1)\n",
      "      (post_dropout_0): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (post_linear_1): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (post_bn_1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (post_activation_1): PReLU(num_parameters=1)\n",
      "      (post_dropout_1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (output_layer): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name              | Type       | Params\n",
      "-------------------------------------------------\n",
      "0 | atom_preprocess   | ModuleList | 109 K \n",
      "1 | cheb_convolutions | ModuleList | 327 K \n",
      "2 | postprocess       | ModuleList | 33.5 K\n",
      "3 | output_layer      | Linear     | 129   \n",
      "-------------------------------------------------\n",
      "471 K     Trainable params\n",
      "0         Non-trainable params\n",
      "471 K     Total params\n",
      "1.885     Total estimated model params size (MB)\n",
      "Metric val_loss improved. New best score: 0.034\n",
      "Metric val_loss improved by 0.004 >= min_delta = 0.0. New best score: 0.030\n",
      "Metric val_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.028\n",
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.027\n",
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.027\n",
      "/home/nikolenko/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "in_features = molecule_dataset[0].x.shape[1]\n",
    "out_features = 1\n",
    "edge_attr_dim = molecule_dataset[0].edge_attr.shape[1]\n",
    "\n",
    "preprocess_hidden_features = [128, 256, 128]\n",
    "postprocess_hidden_features = [128, 128]\n",
    "\n",
    "cheb_hidden_features = [128, 128]\n",
    "K = [10, 10]\n",
    "cheb_normalization = ['sym', 'sym'] # sym rw\n",
    "\n",
    "optimizer_class = Lion\n",
    "learning_rate = 2.2e-5\n",
    "weight_decay = 3e-5\n",
    "step_size = 80\n",
    "gamma = 0.2\n",
    "max_epochs = 100\n",
    "patience = 5\n",
    "batch_size = 128\n",
    "\n",
    "dropout_rates = [0.0] * (len(preprocess_hidden_features) + len(postprocess_hidden_features))\n",
    "activation_fns = [nn.PReLU] * (len(preprocess_hidden_features) + len(postprocess_hidden_features))\n",
    "use_batch_norm = [True] * (len(preprocess_hidden_features) + len(postprocess_hidden_features))\n",
    "\n",
    "model = MoleculeModel(\n",
    "    atom_in_features=in_features,\n",
    "    preprocess_hidden_features=preprocess_hidden_features,\n",
    "    cheb_hidden_features=cheb_hidden_features,\n",
    "    K=K,\n",
    "    cheb_normalizations=cheb_normalization,\n",
    "    dropout_rates=dropout_rates,\n",
    "    activation_fns=activation_fns,\n",
    "    use_batch_norm=use_batch_norm,\n",
    "    postprocess_hidden_features=postprocess_hidden_features,\n",
    "    out_features=out_features,\n",
    "    optimizer_class=optimizer_class,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    step_size=step_size,\n",
    "    gamma=gamma,\n",
    "    batch_size=batch_size,\n",
    "    metric='rmse'\n",
    ")\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "print(\"Model:\\n\", model)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(monitor='val_loss', mode='min', save_top_k=1, verbose=True)\n",
    "early_stop_callback = EarlyStopping(monitor='val_loss', patience=patience, verbose=True, mode='min')\n",
    "timer = Timer()\n",
    "logger = pl.loggers.TensorBoardLogger('tb_logs', name='MolModel')\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=max_epochs,\n",
    "    enable_checkpointing=False,\n",
    "    callbacks=[early_stop_callback, timer],\n",
    "    enable_progress_bar=False,\n",
    "    logger=logger,\n",
    "    accelerator='gpu',\n",
    "    devices=1,\n",
    ")\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-17T10:04:55.930579Z",
     "iopub.status.busy": "2024-03-17T10:04:55.930337Z",
     "iopub.status.idle": "2024-03-17T10:04:55.935278Z",
     "shell.execute_reply": "2024-03-17T10:04:55.934451Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÐÑÐµÐ¼Ñ Ð¾Ð±ÑÑÐµÐ½Ð¸Ñ: 0:00:04\n"
     ]
    }
   ],
   "source": [
    "seconds = timer.time_elapsed()\n",
    "h, m, s = int(seconds // 3600), int((seconds % 3600) // 60), int(seconds % 60)\n",
    "\n",
    "print(f\"ÐÑÐµÐ¼Ñ Ð¾Ð±ÑÑÐµÐ½Ð¸Ñ: {h}:{m:02d}:{s:02d}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MoleculeModel.forward() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/nikolenko/work/gat/ChebConv.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bserv4/home/nikolenko/work/gat/ChebConv.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m evaluate_model(model, data_module)\n",
      "\u001b[1;32m/home/nikolenko/work/gat/ChebConv.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bserv4/home/nikolenko/work/gat/ChebConv.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bserv4/home/nikolenko/work/gat/ChebConv.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m test_dl:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bserv4/home/nikolenko/work/gat/ChebConv.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m         y_hat \u001b[39m=\u001b[39m model(batch\u001b[39m.\u001b[39;49mx, batch\u001b[39m.\u001b[39;49medge_index, batch\u001b[39m.\u001b[39;49medge_attr)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bserv4/home/nikolenko/work/gat/ChebConv.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m         all_pred\u001b[39m.\u001b[39mextend(y_hat\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bserv4/home/nikolenko/work/gat/ChebConv.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m         all_true\u001b[39m.\u001b[39mextend(batch\u001b[39m.\u001b[39my\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch_geom/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: MoleculeModel.forward() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-17T10:05:06.924211Z",
     "iopub.status.busy": "2024-03-17T10:05:06.923987Z",
     "iopub.status.idle": "2024-03-17T10:05:06.989054Z",
     "shell.execute_reply": "2024-03-17T10:05:06.988354Z"
    }
   },
   "outputs": [],
   "source": [
    "def draw_molecule(smiles, predictions):\n",
    "    mol = Chem.AddHs(Chem.MolFromSmiles(smiles))\n",
    "    predictions_rounded = np.round(predictions, 2)\n",
    "\n",
    "    for atom, pred in zip(mol.GetAtoms(), predictions_rounded):\n",
    "        atom.SetProp('atomNote', str(pred))\n",
    "\n",
    "    img = Chem.Draw.MolToImage(mol, size=(600, 600), kekulize=True)\n",
    "    img.show()\n",
    "\n",
    "#smiles = df_results.iloc[0]['smiles']\n",
    "#predictions = df_results.iloc[0]['predictions']\n",
    "\n",
    "#draw_molecule(smiles, predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_geom_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
