{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-17T09:14:18.467992Z",
     "iopub.status.busy": "2024-03-17T09:14:18.467625Z",
     "iopub.status.idle": "2024-03-17T09:14:22.777060Z",
     "shell.execute_reply": "2024-03-17T09:14:22.776210Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "cuda True\n",
      "NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, Timer\n",
    "\n",
    "\n",
    "from lion_pytorch import Lion\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    print(\"cuda\", torch.cuda.is_available())\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pytorch_lightning.trainer.connectors.data_connector\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"lightning_fabric.plugins.environments.slurm\")\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from utils.add_skipatom import add_skipatom_features_to_dataset\n",
    "from utils.utils import save_trial_to_csv, evaluate_model, create_hyperopt_dir, MoleculeDataModule\n",
    "from utils.train import MoleculeModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-17T09:14:22.781643Z",
     "iopub.status.busy": "2024-03-17T09:14:22.781416Z",
     "iopub.status.idle": "2024-03-17T09:15:53.120597Z",
     "shell.execute_reply": "2024-03-17T09:15:53.111858Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = torch.load(f'../data/QM_100.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-17T09:15:53.200256Z",
     "iopub.status.busy": "2024-03-17T09:15:53.199970Z",
     "iopub.status.idle": "2024-03-17T09:15:53.363017Z",
     "shell.execute_reply": "2024-03-17T09:15:53.362347Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GATv2Conv, TransformerConv, ChebConv, SAGEConv\n",
    "from torch_scatter import scatter_mean\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from utils.train import MoleculeModel\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class AtomEdgeInteraction(nn.Module):\n",
    "    def __init__(self, in_features, edge_features, out_features, edge_importance=1.0):\n",
    "        super(AtomEdgeInteraction, self).__init__()\n",
    "        self.edge_importance = edge_importance\n",
    "        self.interaction = nn.Linear(in_features + edge_features, out_features)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        row, col = edge_index\n",
    "        edge_features = edge_attr * self.edge_importance\n",
    "\n",
    "        atom_features = x[row]\n",
    "        combined_features = torch.cat([atom_features, edge_features], dim=-1)\n",
    "\n",
    "        updated_features = self.interaction(combined_features)\n",
    "\n",
    "        x = scatter_mean(updated_features, col, dim=0, dim_size=x.size(0))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, atom_in_features, edge_attr_dim, preprocess_hidden_features, sage_hidden_features, dropout_rates, activation_fns, use_batch_norm, postprocess_hidden_features, out_features):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.atom_preprocess = nn.ModuleList([AtomEdgeInteraction(atom_in_features, edge_attr_dim, preprocess_hidden_features[0])])\n",
    "        for i in range(1, len(preprocess_hidden_features)):\n",
    "            layer = nn.Sequential(\n",
    "                nn.Linear(preprocess_hidden_features[i-1], preprocess_hidden_features[i]),\n",
    "                nn.BatchNorm1d(preprocess_hidden_features[i]) if use_batch_norm[i] else nn.Identity(),\n",
    "                activation_fns[i](),\n",
    "                nn.Dropout(dropout_rates[i])\n",
    "            )\n",
    "            self.atom_preprocess.append(layer)\n",
    "\n",
    "        self.sage_convolutions = nn.ModuleList()\n",
    "        in_channels = preprocess_hidden_features[-1]\n",
    "        for hidden_dim in sage_hidden_features:\n",
    "            self.sage_convolutions.append(SAGEConv(in_channels, hidden_dim))\n",
    "            in_channels = hidden_dim\n",
    "\n",
    "        self.postprocess = nn.ModuleList()\n",
    "        for i in range(len(postprocess_hidden_features)):\n",
    "            layer = nn.Sequential(\n",
    "                nn.Linear(sage_hidden_features[i-1] if i > 0 else sage_hidden_features[-1], postprocess_hidden_features[i]),\n",
    "                nn.BatchNorm1d(postprocess_hidden_features[i]) if use_batch_norm[len(preprocess_hidden_features) + i] else nn.Identity(),\n",
    "                activation_fns[len(preprocess_hidden_features) + i](),\n",
    "                nn.Dropout(dropout_rates[len(preprocess_hidden_features) + i])\n",
    "            )\n",
    "            self.postprocess.append(layer)\n",
    "\n",
    "        self.output_layer = nn.Linear(postprocess_hidden_features[-1], out_features)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x = self.atom_preprocess[0](x, edge_index, edge_attr)\n",
    "        for layer in self.atom_preprocess[1:]:\n",
    "            x = layer(x)\n",
    "\n",
    "        for conv in self.sage_convolutions:\n",
    "            x = F.relu(conv(x, edge_index))\n",
    "\n",
    "        for layer in self.postprocess:\n",
    "            x = layer(x)\n",
    "\n",
    "        return self.output_layer(x).squeeze(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\n",
      " MoleculeModel(\n",
      "  (model_backbone): Model(\n",
      "    (atom_preprocess): ModuleList(\n",
      "      (0): AtomEdgeInteraction(\n",
      "        (interaction): Linear(in_features=747, out_features=128, bias=True)\n",
      "      )\n",
      "      (1-8): 8 x Sequential(\n",
      "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): PReLU(num_parameters=1)\n",
      "        (3): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (sage_convolutions): ModuleList(\n",
      "      (0): SAGEConv(128, 1024, aggr=mean)\n",
      "      (1): SAGEConv(1024, 1024, aggr=mean)\n",
      "    )\n",
      "    (postprocess): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): PReLU(num_parameters=1)\n",
      "        (3): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Linear(in_features=1024, out_features=128, bias=True)\n",
      "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): PReLU(num_parameters=1)\n",
      "        (3): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (output_layer): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type  | Params\n",
      "-----------------------------------------\n",
      "0 | model_backbone | Model | 3.8 M \n",
      "-----------------------------------------\n",
      "3.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.8 M     Total params\n",
      "15.098    Total estimated model params size (MB)\n",
      "/home/nikolenko/.local/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "Metric val_loss improved. New best score: 0.207\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.207\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.207\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.207\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.206\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.206\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.206\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.206\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.206\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.206\n",
      "Monitored metric val_loss did not improve in the last 5 records. Best score: 0.206. Signaling Trainer to stop.\n"
     ]
    }
   ],
   "source": [
    "in_features = dataset[0].x.shape[1]\n",
    "out_features = 1\n",
    "edge_attr_dim = dataset[0].edge_attr.shape[1]\n",
    "\n",
    "batch_size = 512\n",
    "num_workers = 8\n",
    "\n",
    "data_module = MoleculeDataModule(dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "preprocess_hidden_features = [128] * 9\n",
    "sage_hidden_features = [1024, 1024]\n",
    "postprocess_hidden_features = [1024, 128]\n",
    "\n",
    "\n",
    "dropout_rates = [0.0] * (len(preprocess_hidden_features) + len(postprocess_hidden_features))\n",
    "activation_fns = [nn.PReLU] * (len(preprocess_hidden_features) + len(postprocess_hidden_features))\n",
    "use_batch_norm = [True] * (len(preprocess_hidden_features) + len(postprocess_hidden_features))\n",
    "\n",
    "optimizer_class = Lion\n",
    "learning_rate = 2.2e-5\n",
    "weight_decay = 3e-5\n",
    "step_size = 80\n",
    "gamma = 0.2\n",
    "batch_size = 1024\n",
    "metric = 'rmse'\n",
    "\n",
    "backbone = Model(\n",
    "    atom_in_features=in_features,\n",
    "    edge_attr_dim=edge_attr_dim,\n",
    "    preprocess_hidden_features=preprocess_hidden_features,\n",
    "    sage_hidden_features=sage_hidden_features,\n",
    "    dropout_rates=dropout_rates,\n",
    "    activation_fns=activation_fns,\n",
    "    use_batch_norm=use_batch_norm,\n",
    "    postprocess_hidden_features=postprocess_hidden_features,\n",
    "    out_features=out_features\n",
    ")\n",
    "\n",
    "model = MoleculeModel(\n",
    "    model_backbone=backbone,\n",
    "    optimizer_class=optimizer_class,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    step_size=step_size,\n",
    "    gamma=gamma,\n",
    "    batch_size=batch_size,\n",
    "    metric=metric\n",
    ")\n",
    "\n",
    "print(\"Model:\\n\", model)\n",
    "\n",
    "from pytorch_lightning import Trainer, callbacks\n",
    "\n",
    "checkpoint_callback = callbacks.ModelCheckpoint(monitor='val_loss', mode='min', save_top_k=1, verbose=True)\n",
    "early_stop_callback = callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=True, mode='min')\n",
    "timer = callbacks.Timer()\n",
    "logger = pl.loggers.TensorBoardLogger('tb_logs', name='SAGEConv')\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=100,\n",
    "    enable_checkpointing=False,\n",
    "    callbacks=[early_stop_callback, timer],\n",
    "    enable_progress_bar=False,\n",
    "    logger=logger,\n",
    "    accelerator='gpu',\n",
    "    devices=1\n",
    ")\n",
    "\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-17T10:04:55.930579Z",
     "iopub.status.busy": "2024-03-17T10:04:55.930337Z",
     "iopub.status.idle": "2024-03-17T10:04:55.935278Z",
     "shell.execute_reply": "2024-03-17T10:04:55.934451Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время обучения: 0:00:25\n"
     ]
    }
   ],
   "source": [
    "seconds = timer.time_elapsed()\n",
    "h, m, s = int(seconds // 3600), int((seconds % 3600) // 60), int(seconds % 60)\n",
    "\n",
    "print(f\"Время обучения: {h}:{m:02d}:{s:02d}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_geom_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
