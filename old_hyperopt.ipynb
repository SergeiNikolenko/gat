{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import optuna\n",
    "import csv\n",
    "\n",
    "\n",
    "\n",
    "from rdkit import Chem\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturizationParameters:\n",
    "    def __init__(self):\n",
    "        self.max_atomic_num = 100\n",
    "        self.atom_features = {\n",
    "            'atomic_num': list(range(self.max_atomic_num)),\n",
    "            'degree': [0, 1, 2, 3, 4, 5],\n",
    "            'formal_charge': [-1, -2, 1, 2, 0],\n",
    "            'chiral_tag': [0, 1, 2, 3],\n",
    "            'num_Hs': [0, 1, 2, 3, 4],\n",
    "            'hybridization': [\n",
    "                Chem.rdchem.HybridizationType.SP,\n",
    "                Chem.rdchem.HybridizationType.SP2,\n",
    "                Chem.rdchem.HybridizationType.SP3,\n",
    "                Chem.rdchem.HybridizationType.SP3D,\n",
    "                Chem.rdchem.HybridizationType.SP3D2\n",
    "            ],\n",
    "        }\n",
    "        self.atom_fdim = sum(len(choices) + 1 for choices in self.atom_features.values()) + 2\n",
    "\n",
    "def onek_encoding_unk(value, choices):\n",
    "    encoding = [0] * (len(choices) + 1)\n",
    "    index = choices.index(value) if value in choices else -1\n",
    "    encoding[index] = 1\n",
    "    return encoding\n",
    "\n",
    "def atom_features(atom, params):\n",
    "    features = onek_encoding_unk(atom.GetAtomicNum() - 1, params.atom_features['atomic_num']) + \\\n",
    "               onek_encoding_unk(atom.GetTotalDegree(), params.atom_features['degree']) + \\\n",
    "               onek_encoding_unk(atom.GetFormalCharge(), params.atom_features['formal_charge']) + \\\n",
    "               onek_encoding_unk(int(atom.GetChiralTag()), params.atom_features['chiral_tag']) + \\\n",
    "               onek_encoding_unk(int(atom.GetTotalNumHs()), params.atom_features['num_Hs']) + \\\n",
    "               onek_encoding_unk(int(atom.GetHybridization()), params.atom_features['hybridization']) + \\\n",
    "               [1 if atom.GetIsAromatic() else 0] + \\\n",
    "               [atom.GetMass() * 0.01]  # scaled to about the same range as other features\n",
    "    return features\n",
    "\n",
    "PARAMS = {\n",
    "    'BOND_FDIM': 10\n",
    "}\n",
    "\n",
    "def bond_features(bond: Chem.rdchem.Bond) -> List[Union[bool, int, float]]:\n",
    "    if bond is None:\n",
    "        fbond = [1] + [0] * (PARAMS['BOND_FDIM'] - 1)\n",
    "    else:\n",
    "        bt = bond.GetBondType()\n",
    "        fbond = [\n",
    "            0,  # bond is not None\n",
    "            bt == Chem.rdchem.BondType.SINGLE,\n",
    "            bt == Chem.rdchem.BondType.DOUBLE,\n",
    "            bt == Chem.rdchem.BondType.TRIPLE,\n",
    "            bt == Chem.rdchem.BondType.AROMATIC,\n",
    "            bond.GetIsConjugated() if bt is not None else 0,\n",
    "            bond.IsInRing() if bt is not None else 0\n",
    "        ]\n",
    "        fbond += onek_encoding_unk(int(bond.GetStereo()), list(range(6)))\n",
    "    return fbond\n",
    "\n",
    "\n",
    "class MoleculeData:\n",
    "    def __init__(self, smiles, target, addHs=True):\n",
    "        self.smiles = smiles\n",
    "        self.target = torch.tensor(target, dtype=torch.float)\n",
    "        self.mol = Chem.MolFromSmiles(smiles)\n",
    "        if addHs:\n",
    "            self.mol = Chem.AddHs(self.mol)\n",
    "        self.params = FeaturizationParameters()\n",
    "        self.edge_index, self.edge_attr = self.construct_graph()\n",
    "\n",
    "    def construct_graph(self):\n",
    "        edge_index = []\n",
    "        edge_attr = []\n",
    "        for bond in self.mol.GetBonds():\n",
    "            start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "            edge_index.extend([[start, end], [end, start]])\n",
    "            edge_attr.extend([bond_features(bond), bond_features(bond)])  # Добавляем признаки для обеих направлений связи\n",
    "        return torch.tensor(edge_index).t().contiguous(), torch.tensor(edge_attr, dtype=torch.float)\n",
    "\n",
    "    def generate_atom_features(self):\n",
    "        features = []\n",
    "        for atom in self.mol.GetAtoms():\n",
    "            features.append(atom_features(atom, self.params))\n",
    "        return torch.tensor(features, dtype=torch.float)\n",
    "\n",
    "class MoleculeDataset(Dataset):\n",
    "    def __init__(self, dataframe, smiles_column='smiles', target_column='target', addHs=True, n_jobs=-1):\n",
    "        super(MoleculeDataset, self).__init__()\n",
    "        self.data_list = Parallel(n_jobs=n_jobs)(\n",
    "            delayed(lambda row: MoleculeData(row[smiles_column], row[target_column], addHs))(\n",
    "                row) for _, row in tqdm(dataframe.iterrows(), total=dataframe.shape[0]))\n",
    "\n",
    "    def len(self): \n",
    "        return len(self.data_list)\n",
    "\n",
    "    def get(self, idx):\n",
    "        molecule_data = self.data_list[idx]\n",
    "        x = molecule_data.generate_atom_features()\n",
    "        edge_index = molecule_data.edge_index\n",
    "        edge_attr = molecule_data.edge_attr\n",
    "        y = molecule_data.target\n",
    "        \n",
    "        data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "        data.smiles = molecule_data.smiles\n",
    "        \n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "molecule_dataset = torch.load('../data/QM_137k.pt')\n",
    "print(molecule_dataset)\n",
    "for i in range(2):\n",
    "    print(molecule_dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(len(molecule_dataset)))\n",
    "train_val_indices, test_indices = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "train_indices, val_indices = train_test_split(train_val_indices, test_size=0.125, random_state=42)  # 0.125 * 0.8 = 0.1\n",
    "\n",
    "train_dataset = [molecule_dataset[i] for i in tqdm(train_indices, desc=\"Creating Train Dataset\")]\n",
    "val_dataset = [molecule_dataset[i] for i in tqdm(val_indices, desc=\"Creating Validation Dataset\")]\n",
    "test_dataset = [molecule_dataset[i] for i in tqdm(test_indices, desc=\"Creating Test Dataset\")]\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "dataset = DataLoader(molecule_dataset, batch_size=batch_size)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)\n",
    "\n",
    "\n",
    "in_features = 133                # Количество входных признаков для каждого узла\n",
    "out_features =  1                # Количество выходных признаков\n",
    "num_epochs = 1000                # Количество эпох обучения\n",
    "learning_rate = 0.005            # Скорость обучения\n",
    "weight_decay = 5e-4              # Вес распада для регуляризации\n",
    "warmup_epochs = 2                \n",
    "initial_lr = learning_rate  \n",
    "patience = 10\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "hidden_features = 64             # Количество скрытых признаков\n",
    "num_heads = 1                    # Количество \"голов\" в механизме внимания\n",
    "dropout_rate = 0.1                 # Процент дропаута\n",
    "\n",
    "# Определение модели\n",
    "class GATv2Model(torch.nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, num_heads, dropout_rate):\n",
    "        super(GATv2Model, self).__init__()\n",
    "        self.conv1 = GATv2Conv(in_channels=in_features, out_channels=hidden_features,\n",
    "                               heads=num_heads, dropout=dropout_rate, concat=True)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_features * num_heads)\n",
    "        self.conv2 = GATv2Conv(in_channels=hidden_features * num_heads, out_channels=out_features, \n",
    "                               heads=1, concat=False, dropout=dropout_rate)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(self.bn1(x))\n",
    "        x = F.dropout(x, training=self.training, p=dropout_rate)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x.squeeze()\n",
    "\n",
    "model = GATv2Model(\n",
    "        in_features=in_features,\n",
    "        hidden_features=hidden_features,\n",
    "        out_features=out_features,\n",
    "        num_heads=num_heads,\n",
    "        dropout_rate=dropout_rate\n",
    "    )\n",
    "\n",
    "model.to(device)\n",
    "print(f\"Model:\\n{model}\")\n",
    "\n",
    "# Определение функции потерь и оптимизатора\n",
    "loss_func = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "print('Start Train')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_trial_to_csv(trial_number, trial_value, trial_params, csv_path='optuna_results.csv'):\n",
    "    with open(csv_path, 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        if csvfile.tell() == 0:\n",
    "            headers = ['Trial', 'Value'] + [key for key in trial_params.keys()]\n",
    "            writer.writerow(headers)\n",
    "        row = [trial_number, trial_value] + list(trial_params.values())\n",
    "        writer.writerow(row)\n",
    "\n",
    "def train_model(trial):\n",
    "    # Гиперпараметры, подлежащие оптимизации\n",
    "    hidden_features = trial.suggest_int('hidden_features', 32, 1024, step=32)\n",
    "    num_heads = trial.suggest_int('num_heads', 1, 9)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-5, 1e-3, log=True)\n",
    "    # еще есть торч лайтнинг\n",
    "\n",
    "    # Создание и обучение модели\n",
    "    model = GATv2Model(in_features, hidden_features, out_features, num_heads, dropout_rate).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        if epoch < warmup_epochs:\n",
    "            lr = learning_rate * (epoch + 1) / warmup_epochs\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "        else:\n",
    "            lr = learning_rate\n",
    "\n",
    "        model.train()\n",
    "        for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False):\n",
    "            batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(batch.x, batch.edge_index)\n",
    "            loss = loss_func(out, batch.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Валидация\n",
    "        model.eval()\n",
    "        val_loss_accum = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc='Validating', leave=False):\n",
    "                batch.to(device)\n",
    "                val_out = model(batch.x, batch.edge_index)\n",
    "                val_loss = loss_func(val_out, batch.y)\n",
    "                val_loss_accum += val_loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss_accum / len(val_loader)\n",
    "        \n",
    "        # Ранняя остановка\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == patience:\n",
    "                break\n",
    "\n",
    "    return best_val_loss\n",
    "\n",
    "def objective(trial):\n",
    "    best_val_loss = train_model(trial)\n",
    "    save_trial_to_csv(trial.number, best_val_loss, trial.params)\n",
    "    return best_val_loss\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print(f' Value: {trial.value}')\n",
    "print(' Params: ')\n",
    "for key, value in trial.params.items():\n",
    "    print(f'    {key}: {value}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemprop-atom-bond",
   "language": "python",
   "name": "chemprop-atom-bond"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
