{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-17T09:14:18.467992Z",
     "iopub.status.busy": "2024-03-17T09:14:18.467625Z",
     "iopub.status.idle": "2024-03-17T09:14:22.777060Z",
     "shell.execute_reply": "2024-03-17T09:14:22.776210Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda True\n",
      "NVIDIA GeForce RTX 3080\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorboard\n",
    "from rdkit import Chem\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, Timer\n",
    "\n",
    "from lion_pytorch import Lion\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"cuda\", torch.cuda.is_available())\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pytorch_lightning.trainer.connectors.data_connector\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"lightning_fabric.plugins.environments.slurm\")\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from utils.train import MoleculeModel, MoleculeDataModule, GATv2Model, evaluate_model\n",
    "from utils.prepare import MoleculeData, MoleculeDataset, FeaturizationParameters\n",
    "\n",
    "#TODO\n",
    "#SAGE попробовать \n",
    "#слой чебышев попробовать\n",
    "#отправить 10к на трейн остальное на тест "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-17T09:14:22.781643Z",
     "iopub.status.busy": "2024-03-17T09:14:22.781416Z",
     "iopub.status.idle": "2024-03-17T09:15:53.120597Z",
     "shell.execute_reply": "2024-03-17T09:15:53.111858Z"
    }
   },
   "outputs": [],
   "source": [
    "molecule_dataset = torch.load(\"../data/QM_137k.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-17T09:15:53.133986Z",
     "iopub.status.busy": "2024-03-17T09:15:53.133560Z",
     "iopub.status.idle": "2024-03-17T09:15:53.142927Z",
     "shell.execute_reply": "2024-03-17T09:15:53.142248Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[31, 133], edge_index=[2, 64], edge_attr=[64, 14], y=[31], smiles='CNC(=S)N/N=C/c1c(O)ccc2ccccc12')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "molecule_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-17T09:15:53.150721Z",
     "iopub.status.busy": "2024-03-17T09:15:53.150370Z",
     "iopub.status.idle": "2024-03-17T09:15:53.154759Z",
     "shell.execute_reply": "2024-03-17T09:15:53.154105Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 128   \n",
    "num_workers = 8  \n",
    "\n",
    "data_module = MoleculeDataModule(molecule_dataset, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATv2Conv\n",
    "from torch_scatter import scatter_mean\n",
    "\n",
    "class GATv2Model(nn.Module):\n",
    "    def __init__(self, atom_in_features, edge_in_features, num_preprocess_layers, preprocess_hidden_features, num_heads, dropout_rates, activation_fns, use_batch_norm, num_postprocess_layers, postprocess_hidden_features, out_features):\n",
    "        super(GATv2Model, self).__init__()\n",
    "\n",
    "        # Preprocessing layers for atom features\n",
    "        self.atom_preprocess = nn.ModuleList()\n",
    "        for i in range(num_preprocess_layers):\n",
    "            preprocess_layer = nn.Sequential()\n",
    "            in_features = atom_in_features if i == 0 else preprocess_hidden_features[i-1]\n",
    "            preprocess_layer.add_module(f'atom_linear_{i}', nn.Linear(in_features, preprocess_hidden_features[i]))\n",
    "            if use_batch_norm[i]:\n",
    "                preprocess_layer.add_module(f'atom_bn_{i}', nn.BatchNorm1d(preprocess_hidden_features[i]))\n",
    "            preprocess_layer.add_module(f'atom_activation_{i}', activation_fns[i]())\n",
    "            preprocess_layer.add_module(f'atom_dropout_{i}', nn.Dropout(dropout_rates[i]))\n",
    "            self.atom_preprocess.append(preprocess_layer)\n",
    "\n",
    "        # Preprocessing layers for edge features\n",
    "        self.edge_preprocess = nn.ModuleList()\n",
    "        for i in range(num_preprocess_layers):\n",
    "            preprocess_layer = nn.Sequential()\n",
    "            in_features = edge_in_features if i == 0 else preprocess_hidden_features[i-1]\n",
    "            preprocess_layer.add_module(f'edge_linear_{i}', nn.Linear(in_features, preprocess_hidden_features[i]))\n",
    "            if use_batch_norm[i]:\n",
    "                preprocess_layer.add_module(f'edge_bn_{i}', nn.BatchNorm1d(preprocess_hidden_features[i]))\n",
    "            preprocess_layer.add_module(f'edge_activation_{i}', activation_fns[i]())\n",
    "            preprocess_layer.add_module(f'edge_dropout_{i}', nn.Dropout(dropout_rates[i]))\n",
    "            self.edge_preprocess.append(preprocess_layer)\n",
    "\n",
    "        # GATv2 convolutional layers\n",
    "        self.gat_convolutions = nn.ModuleList()\n",
    "        for i, num_head in enumerate(num_heads):\n",
    "            gat_layer = GATv2Conv(\n",
    "                in_channels=preprocess_hidden_features[-1] * (2 if i == 0 else num_heads[i - 1]),\n",
    "                out_channels=preprocess_hidden_features[-1],\n",
    "                heads=num_head,\n",
    "                dropout=dropout_rates[num_preprocess_layers + i],\n",
    "                concat=True\n",
    "            )\n",
    "            self.gat_convolutions.add_module(f'gat_conv_{i}', gat_layer)\n",
    "\n",
    "\n",
    "        # Postprocessing layers\n",
    "        self.postprocess = nn.ModuleList()\n",
    "        for i in range(num_postprocess_layers):\n",
    "            post_layer = nn.Sequential()\n",
    "            in_features = preprocess_hidden_features[-1] * num_heads[-1] if i == 0 else postprocess_hidden_features[i-1]\n",
    "            post_layer.add_module(f'post_linear_{i}', nn.Linear(in_features, postprocess_hidden_features[i]))\n",
    "            if use_batch_norm[num_preprocess_layers + len(num_heads) + i]:\n",
    "                post_layer.add_module(f'post_bn_{i}', nn.BatchNorm1d(postprocess_hidden_features[i]))\n",
    "            post_layer.add_module(f'post_activation_{i}', activation_fns[num_preprocess_layers + len(num_heads) + i]())\n",
    "            post_layer.add_module(f'post_dropout_{i}', nn.Dropout(dropout_rates[num_preprocess_layers + len(num_heads) + i]))\n",
    "            self.postprocess.append(post_layer)\n",
    "\n",
    "        self.output_layer = nn.Linear(postprocess_hidden_features[-1], out_features)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        for layer in self.atom_preprocess:\n",
    "            x = layer(x)\n",
    "\n",
    "        for layer in self.edge_preprocess:\n",
    "            edge_attr = layer(edge_attr)\n",
    "\n",
    "        # Combine atom and edge features\n",
    "        row, col = edge_index\n",
    "        aggregated_edge_features = scatter_mean(edge_attr, col, dim=0, dim_size=x.size(0))\n",
    "        x = torch.cat([x, aggregated_edge_features], dim=1)\n",
    "\n",
    "        # Apply GATv2 convolutions\n",
    "        for conv in self.gat_convolutions.children():\n",
    "            x = conv(x, edge_index)\n",
    "        \n",
    "        # Apply postprocessing\n",
    "        for layer in self.postprocess:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.output_layer(x).squeeze(-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-17T09:15:53.200256Z",
     "iopub.status.busy": "2024-03-17T09:15:53.199970Z",
     "iopub.status.idle": "2024-03-17T09:15:53.363017Z",
     "shell.execute_reply": "2024-03-17T09:15:53.362347Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\n",
      " MoleculeModel(\n",
      "  (base_model): GATv2Model(\n",
      "    (atom_preprocess): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (atom_linear_0): Linear(in_features=133, out_features=128, bias=True)\n",
      "        (atom_activation_0): PReLU(num_parameters=1)\n",
      "        (atom_dropout_0): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (atom_linear_1): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (atom_activation_1): PReLU(num_parameters=1)\n",
      "        (atom_dropout_1): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (atom_linear_2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (atom_activation_2): PReLU(num_parameters=1)\n",
      "        (atom_dropout_2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (atom_linear_3): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (atom_activation_3): PReLU(num_parameters=1)\n",
      "        (atom_dropout_3): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (4): Sequential(\n",
      "        (atom_linear_4): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (atom_activation_4): PReLU(num_parameters=1)\n",
      "        (atom_dropout_4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (5): Sequential(\n",
      "        (atom_linear_5): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (atom_activation_5): PReLU(num_parameters=1)\n",
      "        (atom_dropout_5): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (6): Sequential(\n",
      "        (atom_linear_6): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (atom_activation_6): PReLU(num_parameters=1)\n",
      "        (atom_dropout_6): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (7): Sequential(\n",
      "        (atom_linear_7): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (atom_activation_7): PReLU(num_parameters=1)\n",
      "        (atom_dropout_7): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (8): Sequential(\n",
      "        (atom_linear_8): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (atom_activation_8): PReLU(num_parameters=1)\n",
      "        (atom_dropout_8): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (edge_preprocess): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (edge_linear_0): Linear(in_features=14, out_features=128, bias=True)\n",
      "        (edge_activation_0): PReLU(num_parameters=1)\n",
      "        (edge_dropout_0): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (edge_linear_1): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (edge_activation_1): PReLU(num_parameters=1)\n",
      "        (edge_dropout_1): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (edge_linear_2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (edge_activation_2): PReLU(num_parameters=1)\n",
      "        (edge_dropout_2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (edge_linear_3): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (edge_activation_3): PReLU(num_parameters=1)\n",
      "        (edge_dropout_3): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (4): Sequential(\n",
      "        (edge_linear_4): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (edge_activation_4): PReLU(num_parameters=1)\n",
      "        (edge_dropout_4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (5): Sequential(\n",
      "        (edge_linear_5): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (edge_activation_5): PReLU(num_parameters=1)\n",
      "        (edge_dropout_5): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (6): Sequential(\n",
      "        (edge_linear_6): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (edge_activation_6): PReLU(num_parameters=1)\n",
      "        (edge_dropout_6): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (7): Sequential(\n",
      "        (edge_linear_7): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (edge_activation_7): PReLU(num_parameters=1)\n",
      "        (edge_dropout_7): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (8): Sequential(\n",
      "        (edge_linear_8): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (edge_activation_8): PReLU(num_parameters=1)\n",
      "        (edge_dropout_8): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (gat_convolutions): ModuleList(\n",
      "      (0): GATv2Conv(256, 128, heads=16)\n",
      "      (1): GATv2Conv(2048, 128, heads=20)\n",
      "    )\n",
      "    (postprocess): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (post_linear_0): Linear(in_features=2560, out_features=128, bias=True)\n",
      "        (post_activation_0): PReLU(num_parameters=1)\n",
      "        (post_dropout_0): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (post_linear_1): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (post_activation_1): PReLU(num_parameters=1)\n",
      "        (post_dropout_1): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (output_layer): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "in_features = molecule_dataset[0].x.shape[1]\n",
    "hidden_features = [64, 64, 64, 64, 64, 64, 64, 64, 64]  # Размеры предобработки для каждого слоя\n",
    "postprocess_hidden_features = [64, 64]  # Размеры слоёв постобработки\n",
    "#попробовать уменшьаюсщиеся #TODO\n",
    "num_heads = [16, 20]  # Количество голов внимания для каждого слоя GATv2\n",
    "hidden_features = [128, 128, 128, 128, 128, 128, 128, 128, 128]  # Размеры предобработки для каждого слоя\n",
    "postprocess_hidden_features = [128, 128]  # Размеры слоёв постобработки\n",
    "num_heads = [16, 20, 16]  # Количество голов внимания для каждого слоя GATv2\n",
    "\n",
    "edge_attr_dim = molecule_dataset[0].edge_attr.shape[1]\n",
    "\n",
    "dropout_rates = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  \n",
    "\n",
    "\n",
    "activation_fns = [nn.PReLU, nn.PReLU, nn.PReLU, nn.PReLU, nn.PReLU, nn.PReLU, nn.PReLU, nn.PReLU, nn.PReLU, nn.PReLU, nn.PReLU, nn.PReLU, nn.PReLU]\n",
    "\n",
    "use_batch_norm = [True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
    "use_batch_norm = [False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
    "\n",
    "optimizer_class = Lion\n",
    "\n",
    "learning_rate = 2.2e-5\n",
    "weight_decay = 3e-5\n",
    "\n",
    "step_size = 80\n",
    "gamma = 0.2\n",
    "\n",
    "max_epochs = 100\n",
    "patience = 3\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "base_model = GATv2Model(\n",
    "    atom_in_features=in_features,\n",
    "    edge_in_features=edge_attr_dim,\n",
    "    num_preprocess_layers=len(hidden_features),\n",
    "    preprocess_hidden_features=hidden_features,\n",
    "    num_heads=num_heads,\n",
    "    dropout_rates=dropout_rates,\n",
    "    activation_fns=activation_fns,\n",
    "    use_batch_norm=use_batch_norm,\n",
    "    num_postprocess_layers=len(postprocess_hidden_features),\n",
    "    postprocess_hidden_features=postprocess_hidden_features,\n",
    "    out_features=1\n",
    ")\n",
    "\n",
    "model = MoleculeModel(\n",
    "    base_model=base_model,\n",
    "    optimizer_class=optimizer_class,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    step_size=step_size,\n",
    "    batch_size=batch_size,\n",
    "    gamma=gamma,\n",
    "    metric='rmse'\n",
    ")\n",
    "\n",
    "print(\"Model:\\n\", model)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(monitor='val_loss', mode='min', save_top_k=1, verbose=True)\n",
    "early_stop_callback = EarlyStopping(monitor='val_loss', patience=patience, verbose=True, mode='min')\n",
    "timer = Timer()\n",
    "logger = pl.loggers.TensorBoardLogger('logs', name='GATv2')\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=max_epochs,\n",
    "    enable_checkpointing=False,\n",
    "    callbacks=[early_stop_callback, timer],\n",
    "    enable_progress_bar=False,\n",
    "    logger=logger,\n",
    "    accelerator='gpu',\n",
    "    devices=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-17T09:15:53.371607Z",
     "iopub.status.busy": "2024-03-17T09:15:53.371271Z",
     "iopub.status.idle": "2024-03-17T10:04:55.925740Z",
     "shell.execute_reply": "2024-03-17T10:04:55.924885Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | base_model | GATv2Model | 12.2 M\n",
      "------------------------------------------\n",
      "12.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.2 M    Total params\n",
      "48.722    Total estimated model params size (MB)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 1 in argument 0, but got tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/sergei/Documents/gat/train.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bcomputer/home/sergei/Documents/gat/train.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model, data_module)\n",
      "File \u001b[0;32m~/app/anaconda3/envs/ml/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m TrainerStatus\u001b[39m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    545\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    546\u001b[0m )\n",
      "File \u001b[0;32m~/app/anaconda3/envs/ml/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/app/anaconda3/envs/ml/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    574\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    575\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    576\u001b[0m     ckpt_path,\n\u001b[1;32m    577\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m )\n\u001b[0;32m--> 580\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    582\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    583\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/app/anaconda3/envs/ml/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:987\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    982\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    984\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 987\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m    989\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    991\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    992\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/app/anaconda3/envs/ml/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1031\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[1;32m   1030\u001b[0m     \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1031\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sanity_check()\n\u001b[1;32m   1032\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1033\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/app/anaconda3/envs/ml/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1060\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1057\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_start\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1059\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1060\u001b[0m val_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1062\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1064\u001b[0m \u001b[39m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/app/anaconda3/envs/ml/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:182\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     context_manager \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mno_grad\n\u001b[1;32m    181\u001b[0m \u001b[39mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 182\u001b[0m     \u001b[39mreturn\u001b[39;00m loop_run(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/app/anaconda3/envs/ml/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:135\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mis_last_batch \u001b[39m=\u001b[39m data_fetcher\u001b[39m.\u001b[39mdone\n\u001b[1;32m    134\u001b[0m     \u001b[39m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n\u001b[1;32m    136\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[39m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/app/anaconda3/envs/ml/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:396\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    390\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest_step\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    391\u001b[0m step_args \u001b[39m=\u001b[39m (\n\u001b[1;32m    392\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[1;32m    393\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    394\u001b[0m     \u001b[39melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    395\u001b[0m )\n\u001b[0;32m--> 396\u001b[0m output \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39;49m_call_strategy_hook(trainer, hook_name, \u001b[39m*\u001b[39;49mstep_args)\n\u001b[1;32m    398\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[1;32m    400\u001b[0m \u001b[39mif\u001b[39;00m using_dataloader_iter:\n\u001b[1;32m    401\u001b[0m     \u001b[39m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[0;32m~/app/anaconda3/envs/ml/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 309\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    311\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    312\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/app/anaconda3/envs/ml/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:412\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module:\n\u001b[1;32m    411\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_redirection(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module, \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 412\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlightning_module\u001b[39m.\u001b[39;49mvalidation_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/gat/utils/train.py:130\u001b[0m, in \u001b[0;36mMoleculeModel.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalidation_step\u001b[39m(\u001b[39mself\u001b[39m, batch, batch_idx):\n\u001b[0;32m--> 130\u001b[0m     y_hat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(batch\u001b[39m.\u001b[39;49mx, batch\u001b[39m.\u001b[39;49medge_index, batch\u001b[39m.\u001b[39;49medge_attr)\n\u001b[1;32m    131\u001b[0m     val_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetric(batch\u001b[39m.\u001b[39my, y_hat)\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog(\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, val_loss, batch_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/gat/utils/train.py:115\u001b[0m, in \u001b[0;36mMoleculeModel.forward\u001b[0;34m(self, x, edge_index, edge_attr)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, edge_index, edge_attr):\n\u001b[0;32m--> 115\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_model(x, edge_index, edge_attr)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/home/sergei/Documents/gat/train.ipynb Cell 7\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcomputer/home/sergei/Documents/gat/train.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=66'>67</a>\u001b[0m row, col \u001b[39m=\u001b[39m edge_index\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcomputer/home/sergei/Documents/gat/train.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=67'>68</a>\u001b[0m aggregated_edge_features \u001b[39m=\u001b[39m scatter_max(edge_attr, col, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, dim_size\u001b[39m=\u001b[39mx\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m))\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcomputer/home/sergei/Documents/gat/train.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat([x, aggregated_edge_features], dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcomputer/home/sergei/Documents/gat/train.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=70'>71</a>\u001b[0m \u001b[39m# Apply GATv2 convolutions\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcomputer/home/sergei/Documents/gat/train.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=71'>72</a>\u001b[0m \u001b[39mfor\u001b[39;00m conv \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgat_convolutions\u001b[39m.\u001b[39mchildren():\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 1 in argument 0, but got tuple"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-17T10:04:55.930579Z",
     "iopub.status.busy": "2024-03-17T10:04:55.930337Z",
     "iopub.status.idle": "2024-03-17T10:04:55.935278Z",
     "shell.execute_reply": "2024-03-17T10:04:55.934451Z"
    }
   },
   "outputs": [],
   "source": [
    "seconds = timer.time_elapsed()\n",
    "h, m, s = int(seconds // 3600), int((seconds % 3600) // 60), int(seconds % 60)\n",
    "\n",
    "print(f\"Время обучения: {h}:{m:02d}:{s:02d}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-17T10:05:06.924211Z",
     "iopub.status.busy": "2024-03-17T10:05:06.923987Z",
     "iopub.status.idle": "2024-03-17T10:05:06.989054Z",
     "shell.execute_reply": "2024-03-17T10:05:06.988354Z"
    }
   },
   "outputs": [],
   "source": [
    "def draw_molecule(smiles, predictions):\n",
    "    mol = Chem.AddHs(Chem.MolFromSmiles(smiles))\n",
    "    predictions_rounded = np.round(predictions, 2)\n",
    "\n",
    "    for atom, pred in zip(mol.GetAtoms(), predictions_rounded):\n",
    "        atom.SetProp('atomNote', str(pred))\n",
    "\n",
    "    img = Chem.Draw.MolToImage(mol, size=(600, 600), kekulize=True)\n",
    "    img.show()\n",
    "\n",
    "#smiles = df_results.iloc[0]['smiles']\n",
    "#predictions = df_results.iloc[0]['predictions']\n",
    "\n",
    "#draw_molecule(smiles, predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_geom_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
