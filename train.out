GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name              | Type       | Params
-------------------------------------------------
0 | atom_preprocess   | ModuleList | 177 K 
1 | edge_preprocess   | ModuleList | 187 K 
2 | cheb_convolutions | ModuleList | 327 K 
3 | gat_convolutions  | ModuleList | 11.6 M
4 | postprocess       | ModuleList | 344 K 
5 | output_layer      | Linear     | 129   
-------------------------------------------------
12.6 M    Trainable params
0         Non-trainable params
12.6 M    Total params
50.361    Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
cuda True
NVIDIA GeForce RTX 3090
Model:
 MoleculeModel(
  (atom_preprocess): ModuleList(
    (0): Sequential(
      (atom_linear_0): Linear(in_features=333, out_features=128, bias=True)
      (atom_bn_0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (atom_activation_0): PReLU(num_parameters=1)
      (atom_dropout_0): Dropout(p=0.0, inplace=False)
    )
    (1): Sequential(
      (atom_linear_1): Linear(in_features=128, out_features=128, bias=True)
      (atom_bn_1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (atom_activation_1): PReLU(num_parameters=1)
      (atom_dropout_1): Dropout(p=0.0, inplace=False)
    )
    (2): Sequential(
      (atom_linear_2): Linear(in_features=128, out_features=128, bias=True)
      (atom_bn_2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (atom_activation_2): PReLU(num_parameters=1)
      (atom_dropout_2): Dropout(p=0.0, inplace=False)
    )
    (3): Sequential(
      (atom_linear_3): Linear(in_features=128, out_features=128, bias=True)
      (atom_bn_3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (atom_activation_3): PReLU(num_parameters=1)
      (atom_dropout_3): Dropout(p=0.0, inplace=False)
    )
    (4): Sequential(
      (atom_linear_4): Linear(in_features=128, out_features=128, bias=True)
      (atom_bn_4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (atom_activation_4): PReLU(num_parameters=1)
      (atom_dropout_4): Dropout(p=0.0, inplace=False)
    )
    (5): Sequential(
      (atom_linear_5): Linear(in_features=128, out_features=128, bias=True)
      (atom_bn_5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (atom_activation_5): PReLU(num_parameters=1)
      (atom_dropout_5): Dropout(p=0.0, inplace=False)
    )
    (6): Sequential(
      (atom_linear_6): Linear(in_features=128, out_features=128, bias=True)
      (atom_bn_6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (atom_activation_6): PReLU(num_parameters=1)
      (atom_dropout_6): Dropout(p=0.0, inplace=False)
    )
    (7): Sequential(
      (atom_linear_7): Linear(in_features=128, out_features=128, bias=True)
      (atom_bn_7): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (atom_activation_7): PReLU(num_parameters=1)
      (atom_dropout_7): Dropout(p=0.0, inplace=False)
    )
    (8): Sequential(
      (atom_linear_8): Linear(in_features=128, out_features=128, bias=True)
      (atom_bn_8): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (atom_activation_8): PReLU(num_parameters=1)
      (atom_dropout_8): Dropout(p=0.0, inplace=False)
    )
  )
  (edge_preprocess): ModuleList(
    (0): Sequential(
      (edge_linear_0): Linear(in_features=414, out_features=128, bias=True)
      (edge_bn_0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (edge_activation_0): PReLU(num_parameters=1)
      (edge_dropout_0): Dropout(p=0.0, inplace=False)
    )
    (1): Sequential(
      (edge_linear_1): Linear(in_features=128, out_features=128, bias=True)
      (edge_bn_1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (edge_activation_1): PReLU(num_parameters=1)
      (edge_dropout_1): Dropout(p=0.0, inplace=False)
    )
    (2): Sequential(
      (edge_linear_2): Linear(in_features=128, out_features=128, bias=True)
      (edge_bn_2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (edge_activation_2): PReLU(num_parameters=1)
      (edge_dropout_2): Dropout(p=0.0, inplace=False)
    )
    (3): Sequential(
      (edge_linear_3): Linear(in_features=128, out_features=128, bias=True)
      (edge_bn_3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (edge_activation_3): PReLU(num_parameters=1)
      (edge_dropout_3): Dropout(p=0.0, inplace=False)
    )
    (4): Sequential(
      (edge_linear_4): Linear(in_features=128, out_features=128, bias=True)
      (edge_bn_4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (edge_activation_4): PReLU(num_parameters=1)
      (edge_dropout_4): Dropout(p=0.0, inplace=False)
    )
    (5): Sequential(
      (edge_linear_5): Linear(in_features=128, out_features=128, bias=True)
      (edge_bn_5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (edge_activation_5): PReLU(num_parameters=1)
      (edge_dropout_5): Dropout(p=0.0, inplace=False)
    )
    (6): Sequential(
      (edge_linear_6): Linear(in_features=128, out_features=128, bias=True)
      (edge_bn_6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (edge_activation_6): PReLU(num_parameters=1)
      (edge_dropout_6): Dropout(p=0.0, inplace=False)
    )
    (7): Sequential(
      (edge_linear_7): Linear(in_features=128, out_features=128, bias=True)
      (edge_bn_7): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (edge_activation_7): PReLU(num_parameters=1)
      (edge_dropout_7): Dropout(p=0.0, inplace=False)
    )
    (8): Sequential(
      (edge_linear_8): Linear(in_features=128, out_features=128, bias=True)
      (edge_bn_8): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (edge_activation_8): PReLU(num_parameters=1)
      (edge_dropout_8): Dropout(p=0.0, inplace=False)
    )
  )
  (cheb_convolutions): ModuleList(
    (0-1): 2 x ChebConv(128, 128, K=10, normalization=sym)
  )
  (gat_convolutions): ModuleList(
    (0): GATv2Conv(256, 128, heads=16)
    (1): GATv2Conv(2048, 128, heads=20)
  )
  (postprocess): ModuleList(
    (0): Sequential(
      (post_linear_0): Linear(in_features=2560, out_features=128, bias=True)
      (post_bn_0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (post_activation_0): PReLU(num_parameters=1)
      (post_dropout_0): Dropout(p=0.0, inplace=False)
    )
    (1): Sequential(
      (post_linear_1): Linear(in_features=128, out_features=128, bias=True)
      (post_bn_1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (post_activation_1): PReLU(num_parameters=1)
      (post_dropout_1): Dropout(p=0.0, inplace=False)
    )
  )
  (output_layer): Linear(in_features=128, out_features=1, bias=True)
)
/home/nikolenko/.local/lib/python3.10/site-packages/pytorch_lightning/core/module.py:507: You called `self.log('val_loss', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`
/home/nikolenko/.local/lib/python3.10/site-packages/pytorch_lightning/core/module.py:507: You called `self.log('train_loss', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`
Metric val_loss improved. New best score: 0.032
Metric val_loss improved by 0.004 >= min_delta = 0.0. New best score: 0.028
Metric val_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.026
Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.025
Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.023
Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.023
Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.022
