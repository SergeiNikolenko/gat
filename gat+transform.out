GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name                     | Type       | Params
--------------------------------------------------------
0 | atom_preprocess          | ModuleList | 151 K 
1 | edge_preprocess          | ModuleList | 136 K 
2 | gat_convolutions         | ModuleList | 11.6 M
3 | transformer_convolutions | ModuleList | 23.1 M
4 | gat_postprocess          | ModuleList | 344 K 
5 | transformer_postprocess  | ModuleList | 344 K 
6 | output_layer             | Linear     | 257   
--------------------------------------------------------
35.6 M    Trainable params
0         Non-trainable params
35.6 M    Total params
142.471   Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
cuda True
NVIDIA GeForce RTX 2080 SUPER
Model:
 MoleculeModel(
  (atom_preprocess): ModuleList(
    (0): Sequential(
      (atom_linear_0): Linear(in_features=133, out_features=128, bias=True)
      (atom_bn_0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (atom_activation_0): PReLU(num_parameters=1)
      (atom_dropout_0): Dropout(p=0.0, inplace=False)
    )
    (1): Sequential(
      (atom_linear_1): Linear(in_features=128, out_features=128, bias=True)
      (atom_bn_1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (atom_activation_1): PReLU(num_parameters=1)
      (atom_dropout_1): Dropout(p=0.0, inplace=False)
    )
    (2): Sequential(
      (atom_linear_2): Linear(in_features=128, out_features=128, bias=True)
      (atom_bn_2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (atom_activation_2): PReLU(num_parameters=1)
      (atom_dropout_2): Dropout(p=0.0, inplace=False)
    )
    (3): Sequential(
      (atom_linear_3): Linear(in_features=128, out_features=128, bias=True)
      (atom_bn_3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (atom_activation_3): PReLU(num_parameters=1)
      (atom_dropout_3): Dropout(p=0.0, inplace=False)
    )
    (4): Sequential(
      (atom_linear_4): Linear(in_features=128, out_features=128, bias=True)
      (atom_bn_4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (atom_activation_4): PReLU(num_parameters=1)
      (atom_dropout_4): Dropout(p=0.0, inplace=False)
    )
    (5): Sequential(
      (atom_linear_5): Linear(in_features=128, out_features=128, bias=True)
      (atom_bn_5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (atom_activation_5): PReLU(num_parameters=1)
      (atom_dropout_5): Dropout(p=0.0, inplace=False)
    )
    (6): Sequential(
      (atom_linear_6): Linear(in_features=128, out_features=128, bias=True)
      (atom_bn_6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (atom_activation_6): PReLU(num_parameters=1)
      (atom_dropout_6): Dropout(p=0.0, inplace=False)
    )
    (7): Sequential(
      (atom_linear_7): Linear(in_features=128, out_features=128, bias=True)
      (atom_bn_7): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (atom_activation_7): PReLU(num_parameters=1)
      (atom_dropout_7): Dropout(p=0.0, inplace=False)
    )
    (8): Sequential(
      (atom_linear_8): Linear(in_features=128, out_features=128, bias=True)
      (atom_bn_8): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (atom_activation_8): PReLU(num_parameters=1)
      (atom_dropout_8): Dropout(p=0.0, inplace=False)
    )
  )
  (edge_preprocess): ModuleList(
    (0): Sequential(
      (edge_linear_0): Linear(in_features=14, out_features=128, bias=True)
      (edge_bn_0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (edge_activation_0): PReLU(num_parameters=1)
      (edge_dropout_0): Dropout(p=0.0, inplace=False)
    )
    (1): Sequential(
      (edge_linear_1): Linear(in_features=128, out_features=128, bias=True)
      (edge_bn_1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (edge_activation_1): PReLU(num_parameters=1)
      (edge_dropout_1): Dropout(p=0.0, inplace=False)
    )
    (2): Sequential(
      (edge_linear_2): Linear(in_features=128, out_features=128, bias=True)
      (edge_bn_2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (edge_activation_2): PReLU(num_parameters=1)
      (edge_dropout_2): Dropout(p=0.0, inplace=False)
    )
    (3): Sequential(
      (edge_linear_3): Linear(in_features=128, out_features=128, bias=True)
      (edge_bn_3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (edge_activation_3): PReLU(num_parameters=1)
      (edge_dropout_3): Dropout(p=0.0, inplace=False)
    )
    (4): Sequential(
      (edge_linear_4): Linear(in_features=128, out_features=128, bias=True)
      (edge_bn_4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (edge_activation_4): PReLU(num_parameters=1)
      (edge_dropout_4): Dropout(p=0.0, inplace=False)
    )
    (5): Sequential(
      (edge_linear_5): Linear(in_features=128, out_features=128, bias=True)
      (edge_bn_5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (edge_activation_5): PReLU(num_parameters=1)
      (edge_dropout_5): Dropout(p=0.0, inplace=False)
    )
    (6): Sequential(
      (edge_linear_6): Linear(in_features=128, out_features=128, bias=True)
      (edge_bn_6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (edge_activation_6): PReLU(num_parameters=1)
      (edge_dropout_6): Dropout(p=0.0, inplace=False)
    )
    (7): Sequential(
      (edge_linear_7): Linear(in_features=128, out_features=128, bias=True)
      (edge_bn_7): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (edge_activation_7): PReLU(num_parameters=1)
      (edge_dropout_7): Dropout(p=0.0, inplace=False)
    )
    (8): Sequential(
      (edge_linear_8): Linear(in_features=128, out_features=128, bias=True)
      (edge_bn_8): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (edge_activation_8): PReLU(num_parameters=1)
      (edge_dropout_8): Dropout(p=0.0, inplace=False)
    )
  )
  (gat_convolutions): ModuleList(
    (gat_conv_0): GATv2Conv(256, 128, heads=16)
    (gat_conv_1): GATv2Conv(2048, 128, heads=20)
  )
  (transformer_convolutions): ModuleList(
    (transformer_conv_0): TransformerConv(256, 128, heads=16)
    (transformer_conv_1): TransformerConv(2048, 128, heads=20)
  )
  (gat_postprocess): ModuleList(
    (0): Sequential(
      (post_linear_0): Linear(in_features=2560, out_features=128, bias=True)
      (post_bn_0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (post_activation_0): PReLU(num_parameters=1)
      (post_dropout_0): Dropout(p=0.0, inplace=False)
    )
    (1): Sequential(
      (post_linear_1): Linear(in_features=128, out_features=128, bias=True)
      (post_bn_1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (post_activation_1): PReLU(num_parameters=1)
      (post_dropout_1): Dropout(p=0.0, inplace=False)
    )
  )
  (transformer_postprocess): ModuleList(
    (0): Sequential(
      (post_linear_0): Linear(in_features=2560, out_features=128, bias=True)
      (post_bn_0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (post_activation_0): PReLU(num_parameters=1)
      (post_dropout_0): Dropout(p=0.0, inplace=False)
    )
    (1): Sequential(
      (post_linear_1): Linear(in_features=128, out_features=128, bias=True)
      (post_bn_1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (post_activation_1): PReLU(num_parameters=1)
      (post_dropout_1): Dropout(p=0.0, inplace=False)
    )
  )
  (output_layer): Linear(in_features=256, out_features=1, bias=True)
)
/opt/anaconda/envs/torch_geom_2/lib/python3.9/site-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('val_loss', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`
  rank_zero_warn(
/opt/anaconda/envs/torch_geom_2/lib/python3.9/site-packages/pytorch_lightning/core/module.py:483: UserWarning: You called `self.log('train_loss', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`
  rank_zero_warn(
Metric val_loss improved. New best score: 0.038
Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.036
Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.036
Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.035
Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.035
Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.035
Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.035
Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.034
Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.034
Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.034
Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.034
Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.034
Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.034
Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.034
Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.034
Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.034
Monitored metric val_loss did not improve in the last 5 records. Best score: 0.034. Signaling Trainer to stop.
Время обучения: 1:31:17
Test RMSE: 0.0339
Test R²: 0.9735
