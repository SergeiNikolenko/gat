{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda True\n",
      "NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, Timer\n",
    "\n",
    "\n",
    "from lion_pytorch import Lion\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    print(\"cuda\", torch.cuda.is_available())\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pytorch_lightning.trainer.connectors.data_connector\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"lightning_fabric.plugins.environments.slurm\")\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from utils.add_skipatom import add_skipatom_features_to_dataset\n",
    "from utils.utils import save_trial_to_csv, evaluate_model, create_hyperopt_dir, MoleculeDataModule\n",
    "from utils.train import MoleculeModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.load(\"../data/QM_100.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GATv2Conv, TransformerConv, ChebConv\n",
    "from torch_scatter import scatter_mean\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from utils.train import MoleculeModel\n",
    "\n",
    "\n",
    "\n",
    "class AtomEdgeInteraction(nn.Module):\n",
    "    def __init__(self, in_features, edge_features, out_features, edge_importance=1.0):\n",
    "        super(AtomEdgeInteraction, self).__init__()\n",
    "        self.edge_importance = edge_importance\n",
    "        self.interaction = nn.Linear(in_features + edge_features, out_features)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # Получение атрибутов связей для каждой связи в графе\n",
    "        row, col = edge_index\n",
    "        edge_features = edge_attr * self.edge_importance\n",
    "\n",
    "        # Комбинирование атрибутов атомов с атрибутами связей\n",
    "        atom_features = x[row]  # Атрибуты исходящих атомов\n",
    "        combined_features = torch.cat([atom_features, edge_features], dim=-1)\n",
    "\n",
    "        # Применение слоя для комбинированных атрибутов\n",
    "        updated_features = self.interaction(combined_features)\n",
    "\n",
    "        # Обновление атрибутов атомов\n",
    "        x = scatter_mean(updated_features, col, dim=0, dim_size=x.size(0))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, atom_in_features, edge_attr_dim, preprocess_hidden_features, cheb_hidden_features, K, cheb_normalizations, dropout_rates, activation_fns, use_batch_norm, postprocess_hidden_features, out_features):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.atom_preprocess = nn.ModuleList([AtomEdgeInteraction(atom_in_features, edge_attr_dim, preprocess_hidden_features[0])])\n",
    "        for i in range(1, len(preprocess_hidden_features)):\n",
    "            layer = nn.Sequential(\n",
    "                nn.Linear(preprocess_hidden_features[i-1], preprocess_hidden_features[i]),\n",
    "                nn.BatchNorm1d(preprocess_hidden_features[i]) if use_batch_norm[i] else nn.Identity(),\n",
    "                activation_fns[i](),\n",
    "                nn.Dropout(dropout_rates[i])\n",
    "            )\n",
    "            self.atom_preprocess.append(layer)\n",
    "\n",
    "        self.cheb_convolutions = nn.ModuleList()\n",
    "        in_channels = preprocess_hidden_features[-1]\n",
    "        for i in range(len(cheb_hidden_features)):\n",
    "            self.cheb_convolutions.append(ChebConv(in_channels, cheb_hidden_features[i], K[i], normalization=cheb_normalizations[i]))\n",
    "            in_channels = cheb_hidden_features[i]\n",
    "\n",
    "        self.postprocess = nn.ModuleList()\n",
    "        for i in range(len(postprocess_hidden_features)):\n",
    "            layer = nn.Sequential(\n",
    "                nn.Linear(cheb_hidden_features[i-1] if i > 0 else cheb_hidden_features[-1], postprocess_hidden_features[i]),\n",
    "                nn.BatchNorm1d(postprocess_hidden_features[i]) if use_batch_norm[len(preprocess_hidden_features) + i] else nn.Identity(),\n",
    "                activation_fns[len(preprocess_hidden_features) + i](),\n",
    "                nn.Dropout(dropout_rates[len(preprocess_hidden_features) + i])\n",
    "            )\n",
    "            self.postprocess.append(layer)\n",
    "\n",
    "        self.output_layer = nn.Linear(postprocess_hidden_features[-1], out_features)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x = self.atom_preprocess[0](x, edge_index, edge_attr)\n",
    "        for layer in self.atom_preprocess[1:]:\n",
    "            x = layer(x)\n",
    "\n",
    "        for conv in self.cheb_convolutions:\n",
    "            x = F.relu(conv(x, edge_index))\n",
    "\n",
    "        for layer in self.postprocess:\n",
    "            x = layer(x)\n",
    "\n",
    "        return self.output_layer(x).squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Гиперпараметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results will be saved in: hyperopt_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-08 09:45:21,502] A new study created in memory with name: no-name-87247e11-6162-4678-b9cf-c9932a567e15\n",
      "/tmp/ipykernel_2495598/3857961455.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-2)\n",
      "/tmp/ipykernel_2495598/3857961455.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-2)\n",
      "/tmp/ipykernel_2495598/3857961455.py:20: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  gamma = trial.suggest_uniform('gamma', 0.1, 0.9)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/nikolenko/.local/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "[I 2024-05-08 09:45:30,286] Trial 0 finished with value: 0.25865599513053894 and parameters: {'learning_rate': 0.00013694643286050532, 'weight_decay': 1.2794844183041193e-06, 'step_size': 100, 'gamma': 0.28913830933574525, 'hidden_size': 384}. Best is trial 0 with value: 0.25865599513053894.\n",
      "/tmp/ipykernel_2495598/3857961455.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-2)\n",
      "/tmp/ipykernel_2495598/3857961455.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-2)\n",
      "/tmp/ipykernel_2495598/3857961455.py:20: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  gamma = trial.suggest_uniform('gamma', 0.1, 0.9)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[I 2024-05-08 09:45:42,808] Trial 1 finished with value: 0.3852567970752716 and parameters: {'learning_rate': 0.0002590252739705458, 'weight_decay': 3.804339673295596e-06, 'step_size': 20, 'gamma': 0.8872947397554413, 'hidden_size': 1024}. Best is trial 0 with value: 0.25865599513053894.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[I 2024-05-08 09:45:54,000] Trial 2 finished with value: 1.4441970586776733 and parameters: {'learning_rate': 0.0014342771310984497, 'weight_decay': 0.0012521711193552061, 'step_size': 80, 'gamma': 0.2699954411011172, 'hidden_size': 640}. Best is trial 0 with value: 0.25865599513053894.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[I 2024-05-08 09:46:02,122] Trial 3 finished with value: 8.320130348205566 and parameters: {'learning_rate': 0.007907838571837638, 'weight_decay': 0.0011989251177798135, 'step_size': 140, 'gamma': 0.1690120748884768, 'hidden_size': 128}. Best is trial 0 with value: 0.25865599513053894.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[I 2024-05-08 09:46:34,658] Trial 4 finished with value: 0.20255614817142487 and parameters: {'learning_rate': 0.0003581278980240764, 'weight_decay': 0.0007604359987676776, 'step_size': 100, 'gamma': 0.6429245971394766, 'hidden_size': 128}. Best is trial 4 with value: 0.20255614817142487.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, Timer\n",
    "import torch.nn as nn\n",
    "\n",
    "def objective(trial):\n",
    "    try:\n",
    "        # Static configuration from the dataset\n",
    "        in_features = dataset[0].x.shape[1]\n",
    "        out_features = 1\n",
    "        edge_attr_dim = dataset[0].edge_attr.shape[1]\n",
    "        optimizer_class = Lion\n",
    "        metric = 'rmse'\n",
    "\n",
    "        # Dynamic parameters to optimize using Optuna\n",
    "        learning_rate = trial.suggest_float('learning_rate', 1e-6, 1e-2, log=True)\n",
    "        weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-2,log=True)\n",
    "        step_size = trial.suggest_int('step_size', 20, 160, step=20)\n",
    "        gamma = trial.suggest_uniform('gamma', 0.1, 0.9)\n",
    "\n",
    "        preprocess_hidden_features = [trial.suggest_int('hidden_size', 128, 1024, step=128)] * 9\n",
    "        postprocess_hidden_features = [trial.suggest_int('hidden_size', 128, 1024, step=128)] * 2\n",
    "\n",
    "        cheb_hidden_features = [\n",
    "            trial.suggest_int('hidden_size', 128, 1024, step=128),\n",
    "            trial.suggest_int('hidden_size', 128, 1024, step=128)\n",
    "        ]\n",
    "        cheb_normalization = ['sym', 'sym']\n",
    "        K = [10, 16]\n",
    "\n",
    "        dropout_rates = [0.0] * (len(preprocess_hidden_features) + len(postprocess_hidden_features))\n",
    "        activation_fns = [nn.PReLU] * (len(preprocess_hidden_features) + len(postprocess_hidden_features))\n",
    "        use_batch_norm = [True] * (len(preprocess_hidden_features) + len(postprocess_hidden_features))\n",
    "\n",
    "        batch_size = 512\n",
    "\n",
    "        backbone = Model(\n",
    "            atom_in_features=in_features,\n",
    "            edge_attr_dim=edge_attr_dim,\n",
    "            preprocess_hidden_features=preprocess_hidden_features,\n",
    "            cheb_hidden_features=cheb_hidden_features,\n",
    "            K=K,\n",
    "            cheb_normalizations=cheb_normalization,\n",
    "            dropout_rates=dropout_rates,\n",
    "            activation_fns=activation_fns,\n",
    "            use_batch_norm=use_batch_norm,\n",
    "            postprocess_hidden_features=postprocess_hidden_features,\n",
    "            out_features=out_features\n",
    "        )\n",
    "\n",
    "        model = MoleculeModel(\n",
    "            model_backbone=backbone,\n",
    "            optimizer_class=optimizer_class,\n",
    "            learning_rate=learning_rate,\n",
    "            weight_decay=weight_decay,\n",
    "            step_size=step_size,\n",
    "            gamma=gamma,\n",
    "            batch_size=batch_size,\n",
    "            metric=metric\n",
    "        )\n",
    "\n",
    "        data_module = MoleculeDataModule(dataset, batch_size=batch_size, num_workers=8)\n",
    "\n",
    "        early_stop_callback = EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\")\n",
    "        timer = Timer()\n",
    "        logger = pl.loggers.TensorBoardLogger('tb_logs', name='hyperopt/full')\n",
    "\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=100,\n",
    "            devices=1,\n",
    "            accelerator='gpu',\n",
    "            logger=logger,\n",
    "            enable_progress_bar=False,\n",
    "            enable_checkpointing=False,\n",
    "            enable_model_summary=False,\n",
    "            callbacks=[early_stop_callback, timer]\n",
    "        )\n",
    "        trainer.fit(model, data_module)\n",
    "\n",
    "        val_loss = trainer.callback_metrics[\"val_loss\"].item()\n",
    "        save_trial_to_csv(trial, hyperopt_dir, val_loss)\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        if 'CUDA out of memory' in str(e):\n",
    "            print(\"CUDA out of memory. Skipping this trial.\")\n",
    "            return float('inf')\n",
    "        raise\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "hyperopt_dir = create_hyperopt_dir()\n",
    "print(f\"Results will be saved in: {hyperopt_dir}\")\n",
    "\n",
    "study = optuna.create_study(direction='minimize', pruner=optuna.pruners.SuccessiveHalvingPruner())\n",
    "study.optimize(objective, n_trials=1000)\n",
    "\n",
    "print(f'Best trial: {study.best_trial.number}')\n",
    "print(f'Best value (RMSE): {study.best_trial.value}')\n",
    "for key, value in study.best_trial.params.items():\n",
    "    print(f'{key}: {value}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_geom_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
