/var/spool/slurmd/job00077/slurm_script: line 15: module: command not found
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
2024-03-28 22:21:30.251470: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name             | Type       | Params
------------------------------------------------
0 | atom_preprocess  | ModuleList | 151 K 
1 | edge_preprocess  | ModuleList | 136 K 
2 | gat_convolutions | ModuleList | 23.1 M
3 | postprocess      | ModuleList | 344 K 
4 | output_layer     | Linear     | 129   
------------------------------------------------
23.7 M    Trainable params
0         Non-trainable params
23.7 M    Total params
94.880    Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
cuda True
NVIDIA GeForce RTX 3080
Model:
 MoleculeModel(
  (atom_preprocess): ModuleList(
    (0): Sequential(
      (atom_linear_0): Linear(in_features=133, out_features=128, bias=True)
      (atom_bn_0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (atom_activation_0): PReLU(num_parameters=1)
      (atom_dropout_0): Dropout(p=0.0, inplace=False)
    )
    (1): Sequential(
      (atom_linear_1): Linear(in_features=128, out_features=128, bias=True)
      (atom_bn_1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (atom_activation_1): PReLU(num_parameters=1)
      (atom_dropout_1): Dropout(p=0.0, inplace=False)
    )
    (2): Sequential(
      (atom_linear_2): Linear(in_features=128, out_features=128, bias=True)
      (atom_bn_2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (atom_activation_2): PReLU(num_parameters=1)
      (atom_dropout_2): Dropout(p=0.0, inplace=False)
    )
    (3): Sequential(
      (atom_linear_3): Linear(in_features=128, out_features=128, bias=True)
      (atom_bn_3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (atom_activation_3): PReLU(num_parameters=1)
      (atom_dropout_3): Dropout(p=0.0, inplace=False)
    )
    (4): Sequential(
      (atom_linear_4): Linear(in_features=128, out_features=128, bias=True)
      (atom_bn_4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (atom_activation_4): PReLU(num_parameters=1)
      (atom_dropout_4): Dropout(p=0.0, inplace=False)
    )
    (5): Sequential(
      (atom_linear_5): Linear(in_features=128, out_features=128, bias=True)
      (atom_bn_5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (atom_activation_5): PReLU(num_parameters=1)
      (atom_dropout_5): Dropout(p=0.0, inplace=False)
    )
    (6): Sequential(
      (atom_linear_6): Linear(in_features=128, out_features=128, bias=True)
      (atom_bn_6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (atom_activation_6): PReLU(num_parameters=1)
      (atom_dropout_6): Dropout(p=0.0, inplace=False)
    )
    (7): Sequential(
      (atom_linear_7): Linear(in_features=128, out_features=128, bias=True)
      (atom_bn_7): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (atom_activation_7): PReLU(num_parameters=1)
      (atom_dropout_7): Dropout(p=0.0, inplace=False)
    )
    (8): Sequential(
      (atom_linear_8): Linear(in_features=128, out_features=128, bias=True)
      (atom_bn_8): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (atom_activation_8): PReLU(num_parameters=1)
      (atom_dropout_8): Dropout(p=0.0, inplace=False)
    )
  )
  (edge_preprocess): ModuleList(
    (0): Sequential(
      (edge_linear_0): Linear(in_features=14, out_features=128, bias=True)
      (edge_bn_0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (edge_activation_0): PReLU(num_parameters=1)
      (edge_dropout_0): Dropout(p=0.0, inplace=False)
    )
    (1): Sequential(
      (edge_linear_1): Linear(in_features=128, out_features=128, bias=True)
      (edge_bn_1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (edge_activation_1): PReLU(num_parameters=1)
      (edge_dropout_1): Dropout(p=0.0, inplace=False)
    )
    (2): Sequential(
      (edge_linear_2): Linear(in_features=128, out_features=128, bias=True)
      (edge_bn_2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (edge_activation_2): PReLU(num_parameters=1)
      (edge_dropout_2): Dropout(p=0.0, inplace=False)
    )
    (3): Sequential(
      (edge_linear_3): Linear(in_features=128, out_features=128, bias=True)
      (edge_bn_3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (edge_activation_3): PReLU(num_parameters=1)
      (edge_dropout_3): Dropout(p=0.0, inplace=False)
    )
    (4): Sequential(
      (edge_linear_4): Linear(in_features=128, out_features=128, bias=True)
      (edge_bn_4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (edge_activation_4): PReLU(num_parameters=1)
      (edge_dropout_4): Dropout(p=0.0, inplace=False)
    )
    (5): Sequential(
      (edge_linear_5): Linear(in_features=128, out_features=128, bias=True)
      (edge_bn_5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (edge_activation_5): PReLU(num_parameters=1)
      (edge_dropout_5): Dropout(p=0.0, inplace=False)
    )
    (6): Sequential(
      (edge_linear_6): Linear(in_features=128, out_features=128, bias=True)
      (edge_bn_6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (edge_activation_6): PReLU(num_parameters=1)
      (edge_dropout_6): Dropout(p=0.0, inplace=False)
    )
    (7): Sequential(
      (edge_linear_7): Linear(in_features=128, out_features=128, bias=True)
      (edge_bn_7): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (edge_activation_7): PReLU(num_parameters=1)
      (edge_dropout_7): Dropout(p=0.0, inplace=False)
    )
    (8): Sequential(
      (edge_linear_8): Linear(in_features=128, out_features=128, bias=True)
      (edge_bn_8): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (edge_activation_8): PReLU(num_parameters=1)
      (edge_dropout_8): Dropout(p=0.0, inplace=False)
    )
  )
  (gat_convolutions): ModuleList(
    (0): TransformerConv(256, 128, heads=16)
    (1): TransformerConv(2048, 128, heads=20)
  )
  (postprocess): ModuleList(
    (0): Sequential(
      (post_linear_0): Linear(in_features=2560, out_features=128, bias=True)
      (post_bn_0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (post_activation_0): PReLU(num_parameters=1)
      (post_dropout_0): Dropout(p=0.0, inplace=False)
    )
    (1): Sequential(
      (post_linear_1): Linear(in_features=128, out_features=128, bias=True)
      (post_bn_1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (post_activation_1): PReLU(num_parameters=1)
      (post_dropout_1): Dropout(p=0.0, inplace=False)
    )
  )
  (output_layer): Linear(in_features=128, out_features=1, bias=True)
)
Metric val_loss improved. New best score: 0.037
Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.036
Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.035
Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.035
Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.035
Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.035
Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.035
Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.034
Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.034
Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.034
Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.034
Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.034
Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.034
Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.034
Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.034
Monitored metric val_loss did not improve in the last 5 records. Best score: 0.034. Signaling Trainer to stop.
Время обучения: 6:54:40
Test RMSE: 0.0340
Test R²: 0.9733
